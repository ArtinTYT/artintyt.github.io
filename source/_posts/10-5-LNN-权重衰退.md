---
title: 5. Weight Decay（权重衰退）
date: 2025-04-22 18:12:30
tags:
    - LNN
    - WeightDecay
    - Regularization
    - Deep_Learning
    - Supervised_Learning
categories:
    - LNNs 
---
# 5. Weight Decay（权重衰退）

- **权重衰退的作用**
	- **权重衰退（Weight Decay）** 是通过在损失函数中加入一个正则化项，来限制模型的容量，防止过拟合。
	- 在训练过程中，模型会学习到较小的参数值，从而避免权重过大。
	- 通过对参数加上惩罚项，使得训练过程中模型学习到的参数不会过大，从而抑制过拟合。

- **权重衰退的公式**
	- 权重衰退通过L2范数来实现，即在目标函数中加入一个参数的L2范数（参数的平方和）作为惩罚项：
	    $$\min_{w,b} \ell(w, b) + \frac{\lambda}{2} \|w\|^2$$
	    - 这里，$\ell(w,b)$ 是原始损失函数，$|w|^2$ 是权重的L2范数，$\lambda$ 是正则化超参数，控制权重衰退的强度。
	    - **$\lambda = 0$** 表示没有正则化，**$\lambda \to \infty$** 时，模型会强迫所有的权重接近0。
        

- **权重衰退对模型训练的影响**
	- **没有权重衰退（无L2正则化）**：
	    - 模型容量可能过大，导致过拟合，训练误差低但测试误差高。
	- **添加权重衰退（L2正则化）**：
	    - 通过正则化惩罚项，模型参数被限制，不会过大，从而降低过拟合风险。
	    - 如果$\lambda$值较大，可能会导致欠拟合，模型难以适应数据。
        

- **权重衰退的图示**
	- 图示展示了没有和有权重衰退的情况：
	    - **左图**：没有权重衰退时，模型容易发生过拟合，训练误差较小但测试误差较大。
	    - **右图**：有权重衰退时，模型参数受到限制，测试误差得到改善。

		<img src="/images/img/img_LNN-weightDecay.png" width=500 style="display: block; margin: 0 auto;"/>

- **使用均方范数作为软性限制**
	- **L2正则化**作为一种软性限制，可以限制参数的大小，但不会完全消除它们。
	- **不对偏置项b进行正则化**：偏置项通常不会受到正则化影响，因为它不影响模型复杂度。
	- 通过调整超参数$\lambda$，可以控制正则化项的影响程度：
	    - **$\lambda = 0$**：没有正则化，过拟合风险较大。
	    - **较小的$\lambda$**：正则化效果较弱，模型有更多自由度，但仍然有限制。
	    - **较大的$\lambda$**：强正则化，模型会倾向于较小的参数，可能导致欠拟合。

		<img src="/images/img/img_LNN-weightDecay2.png" width=280 style="display: block; margin: 0 auto;"/>

- **参数更新规则**
	- **梯度计算**：
	    $$\frac{\partial}{\partial w} \left( \ell(w,b) + \frac{\lambda}{2} \|w\|^2 \right) = \frac{\partial \ell(w,b)}{\partial w} + \lambda w$$
	- **参数更新**：
	    $$w_{t+1} = (1 - \eta \lambda) w_t - \eta \frac{\partial \ell(w_t, b_t)}{\partial w_t}$$
	    
	    - 这里 $\eta$ 是学习率，$\lambda$ 是权重衰退系数。
	    - 通常 $\eta \lambda < 1$，在深度学习中交权重衰退。

## 总结

- 权重衰退通过L2正则化来控制模型的复杂度，防止过拟合。
- 正则化超参数 $\lambda$ 决定了正则化的强度，影响模型的泛化能力。
- 适当的权重衰退可以帮助模型在训练集和测试集上都获得较好的表现。



## 参考资料

- [李沐《手动深度学习》：权重衰退](https://zh.d2l.ai/chapter_multilayer-perceptrons/weight-decay.html)
- [Bilibili：权重衰退](https://www.bilibili.com/video/BV1UK4y1o7dy)

