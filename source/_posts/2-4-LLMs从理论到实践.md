---
title: LLMs基本概念 + 发展历史 + 构建流程
date: 2025-04-19 10:11:10
tags:
    - LLMs
    - Theory
    - AI
    - NLP
categories:
    - LLMs
---

# 大模型绪论

## 1.1 大语言模型的基本概念

- **什么是大语言模型？**
    - 大语言模型（LLM）：指参数量达到数百亿甚至更多的深度神经网络模型，通常使用自监督学习方法，在海量未标注文本上训练。
    - 2018年以来，BERT、GPT等模型相继发布，全面提升NLP任务效果。
    - 2022年11月ChatGPT发布后，LLM能力爆红，大家都知道AI能聊天、写文案、做摘要、翻译，甚至表现出"世界知识"与推理能力。
        
- **语言模型的目标**
    - 语言模型要对自然语言的概率分布建模，本质就是计算一句话出现的概率 $P(w_1, w_2, ..., w_m)$ 。
    - 直接建模联合概率不现实——词表很大，句子很长，参数量呈爆炸式增长（天文数字）。
    - **链式法则**：把联合概率分解为条件概率的连乘。  
        $$P(w_1w_2...w_m) = P(w_1)P(w_2|w_1)P(w_3|w_1w_2)...P(w_m|w_1...w_{m-1})$$
    - 例：  
        $P(\text{把努力变成一种习惯}) = P(\text{把}) \times P(\text{努力}|\text{把}) \times ...$
        
- **n-gram模型的提出**
    - 为减少参数空间，n-gram模型假设每个词只依赖前n-1个词：  
        $$P(w_i∣w_1...w_{i−1})≈P(w_i∣w_{i−n+1}...w_{i−1})$$
    - 这样能极大降低建模难度，但有明显缺陷：
        1. 只能建模有限长度上下文，长距离依赖无能为力。
        2. 必须依赖"平滑技术"解决训练数据中"未见词"n-gram概率为0的问题（如加一平滑、Kneser-Ney平滑等）。
        3. n增大时数据稀疏性加剧，难以获得可靠参数估计。
        4. 词是离散one-hot编码，完全忽略了词与词之间的语义相似性。
            
- **神经网络语言模型（NLM）**
    - 神经网络语言模型（如前馈神经网络、RNN、CNN、记忆网络）成为新主流。
    - 核心思想：用稠密的**词向量（Word Embedding）**替代离散one-hot编码，让相似词的向量距离更近，自动捕获语义关系。
    - 优势：能缓解稀疏性、支持更长距离依赖建模。
    - 训练方式逐步过渡到自监督学习（利用未标注文本直接构造训练目标）。
        
- **预训练语言模型（PLM）与微调范式**
    - 借鉴计算机视觉领域的ImageNet范式，NLP发展出"**预训练+微调**"的主流方案。
    - 先在大规模无标注文本上训练一个通用模型（预训练，学习基础语言知识）；下游任务只需简单微调，无需专门为每个任务重新设计和训练新模型。
    - 代表模型：ELMo（2018，动态词向量）、BERT（2018，编码器结构）、GPT（2018，解码器结构）。
    - PLM时代：每个下游任务都需单独微调。
        
- **大语言模型爆发 & 新范式**
    - GPT-3（2020）参数量达1750亿，掀起"规模=能力"热潮，训练资源消耗极大（上千块GPU，训练数周至数月）。
    - 出现**缩放法则（Scaling Laws）**：模型性能会随参数量、数据量、算力的指数提升而线性改善（损失降低）。
    - 进入**In-Context Learning（ICL，语境学习）**、"Prompt工程"、指令微调、模型即服务（MaaS）等新阶段。
        - ICL：无需模型参数更新，只需输入样例，模型即可迁移执行新任务。
        - Prompt/指令微调等让LLM能更好理解和服从人类意图。


## 1.2 大语言模型的发展历程

- **基础模型阶段（2018年—2021年）**
	- 2017年，Transformer架构提出，成为后续大模型的技术基石。
	- 2018年，Google提出BERT，OpenAI提出GPT-1，预训练语言模型时代拉开序幕。
	    - **参数量迅速增长**：BERT-Base（1.1亿）、BERT-Large（3.4亿）、GPT-1（1.17亿）。
	- 2019年，OpenAI发布GPT-2（15亿参数），Google发布T5（110亿参数）。
	- 2020年，OpenAI进一步扩展到GPT-3（1750亿参数）。
	- 同期，国内发布一批大模型，如清华ERNIE、百度ERNIE、华为PanGu-α等。
	- 这一阶段，各种架构探索如仅编码器（BERT类）、仅解码器（GPT类）、编码器-解码器（T5类）并存，但主流还是预训练-微调范式。
	- **模型规模迈上十亿量级，微调带来的计算量暴涨。**
    
- **能力探索阶段（2019年—2022年）**
	- 由于超大模型微调难度大，研究重心转向不微调参数也能用好大模型（零样本/少样本能力）。
	- 2019年，GPT-2被用于零样本任务探索。
	- GPT-3通过In-Context Learning（ICL，语境学习），只需输入少量有标注样例，无需微调即可解决多种新任务，效果媲美甚至超过传统有监督方法。
	- **出现指令微调**（Instruction Tuning），将多任务转化为统一的生成框架，微调一次即可泛化。
	- 2022年，Ouyang等人提出InstructGPT（有监督微调+奖励建模+强化学习），能让模型更好服从人类指令。
	- 出现WebGPT（结合搜索增强问答），进一步拓宽LLM能力边界。
	- **大模型在多任务上的泛化和实用性大幅提升。**

- **突破发展阶段（2022年11月—2025年2月及以后）**
	- 以2022年11月ChatGPT发布为标志，LLM进入全面爆发期。
	- ChatGPT让LLM通过一个对话框即可支持问答、写作、代码生成、数学推理等复杂任务。
	- 2023年3月，GPT-4发布，显著提升多模态理解与复杂推理能力。
	- 2024年5月，GPT-4o问世，支持文本、音频、图像多模态输入输出，反应速度极快。
	- 2024年9月，GPT-o1等新模型在复杂推理和思维链能力上表现超越前作。
	- 国内外开源和闭源模型数量爆炸式增长：如LLaMA、Qwen、DeepSeek、Baichuan、MOSS、ChatGLM、Gemini、Claude等。
	- **新一代推理增强模型、智能体、多模态LLM成为研究和产业热点。**
    
## 1.3 大语言模型的构建流程（以OpenAI为例）

- LLM训练流程一般分为**四大阶段**，每一阶段对应不同目标和数据类型：
1. **预训练（Pretraining）**
    - 用海量互联网数据（网页、百科、书籍、代码、论文等，数万亿token），在大规模算力集群（数百至数千GPU）上训练出基础模型（Base Model），具备基本的文本生成和世界知识能力。
    - 训练耗时极长，资源消耗巨大（如GPT-3需上千块GPU、数周至数月）。
    - 模型通过"预测下一个词"或"填空"等自监督目标学习语言本质。
        
2. **有监督微调（Supervised Fine-Tuning, SFT）**
    - 用少量高质量标注数据（如QA、对话、翻译等），在基础模型上"微调"，训练得到SFT模型，使其具备更好的指令理解与上下文把控能力。
    - SFT模型能直接用于下游应用，具备初步多任务能力和泛化性。
    - 微调所需数据量远小于预训练阶段，资源消耗较低。
        
3. **奖励建模（Reward Modeling）**
    - 构建奖励模型，对同一输入下SFT模型的多个输出结果进行打分和排序，训练出能自动区分好坏输出的奖励模型。
    - 奖励模型本身**不能直接用于对话或问答**，仅用于后续强化学习阶段的优化目标。
    - 奖励建模对标注质量要求极高，需大规模对比数据，且一致性难以保证，是数据瓶颈之一。
        
4. **强化学习（Reinforcement Learning, RL）**
    - 利用奖励模型给出的反馈信号，对SFT模型参数做进一步优化（如采用PPO算法），提升模型生成内容的质量、相关性与"人类偏好"一致性。
    - RLHF可以显著提升模型的泛化与对话能力，但也可能带来多样性下降、收敛不稳定、超参数难调等新问题。
    - RL阶段训练数据量适中，资源消耗远小于预训练，但比SFT略高。
    - 最终得到的RL模型（如ChatGPT、GPT-4、Claude等）是直接面向用户的产品形态。


## 参考资料

- 《大语言模型：从理论到实践（第二版）》-- 张奇、桂韬、郑锐、黄萱菁
